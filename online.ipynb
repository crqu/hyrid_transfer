{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from rlberry.agents import AgentWithSimplePolicy\n",
    "from rlberry_scool.agents import UCBVIAgent\n",
    "from rlberry_research.envs.benchmarks.grid_exploration.nroom import NRoom\n",
    "from rlberry.manager import(\n",
    "    ExperimentManager,\n",
    "    evaluate_agents,\n",
    "    plot_writer_data,\n",
    "    read_writer_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m[INFO] 10:47:        agent_name  worker  episode_rewards  max_global_step\n",
      "                       UCBVI       -1         0.01              1 \u001b[0m\n",
      "\u001b[38;21m[INFO] 10:52: [UCBVI[worker: -1]] | max_global_step = 2 | episode_rewards = 0.01 | n_visited_states = 16 |  \u001b[0m\n",
      "\u001b[38;21m[INFO] 10:57: [UCBVI[worker: -1]] | max_global_step = 3 | episode_rewards = 0.03 | n_visited_states = 20 |  \u001b[0m\n",
      "\u001b[38;21m[INFO] 11:03: [UCBVI[worker: -1]] | max_global_step = 4 | episode_rewards = 0.04 | n_visited_states = 33 |  \u001b[0m\n",
      "\u001b[38;21m[INFO] 11:08: [UCBVI[worker: -1]] | max_global_step = 5 | episode_rewards = 0.01 | n_visited_states = 36 |  \u001b[0m\n",
      "\u001b[38;21m[INFO] 11:13: [UCBVI[worker: -1]] | max_global_step = 6 | episode_rewards = 0.01 | n_visited_states = 41 |  \u001b[0m\n",
      "\u001b[38;21m[INFO] 11:18: [UCBVI[worker: -1]] | max_global_step = 7 | episode_rewards = 0.01 | n_visited_states = 46 |  \u001b[0m\n",
      "\u001b[38;21m[INFO] 11:23: [UCBVI[worker: -1]] | max_global_step = 8 | episode_rewards = 0.02 | n_visited_states = 49 |  \u001b[0m\n",
      "\u001b[38;21m[INFO] 11:28: [UCBVI[worker: -1]] | max_global_step = 9 | episode_rewards = 0.02 | n_visited_states = 50 |  \u001b[0m\n",
      "\u001b[38;21m[INFO] 11:33: [UCBVI[worker: -1]] | max_global_step = 10 | episode_rewards = 0.01 | n_visited_states = 58 |  \u001b[0m\n",
      "\u001b[38;21m[INFO] 11:38: [UCBVI[worker: -1]] | max_global_step = 11 | episode_rewards = 0.03 | n_visited_states = 66 |  \u001b[0m\n",
      "\u001b[38;21m[INFO] 11:43: [UCBVI[worker: -1]] | max_global_step = 12 | episode_rewards = 0.01 | n_visited_states = 68 |  \u001b[0m\n",
      "\u001b[38;21m[INFO] 11:49: [UCBVI[worker: -1]] | max_global_step = 13 | episode_rewards = 0.01 | n_visited_states = 81 |  \u001b[0m\n",
      "\u001b[38;21m[INFO] 12:10: [UCBVI[worker: -1]] | max_global_step = 14 | episode_rewards = 0.01 | n_visited_states = 82 |  \u001b[0m\n",
      "\u001b[38;21m[INFO] 12:15: [UCBVI[worker: -1]] | max_global_step = 15 | episode_rewards = 0.01 | n_visited_states = 91 |  \u001b[0m\n",
      "\u001b[38;21m[INFO] 12:20: [UCBVI[worker: -1]] | max_global_step = 16 | episode_rewards = 0.01 | n_visited_states = 91 |  \u001b[0m\n",
      "\u001b[38;21m[INFO] 12:25: [UCBVI[worker: -1]] | max_global_step = 17 | episode_rewards = 0.12000000000000001 | n_visited_states = 91 |  \u001b[0m\n",
      "\u001b[38;21m[INFO] 12:31: [UCBVI[worker: -1]] | max_global_step = 18 | episode_rewards = 0.01 | n_visited_states = 96 |  \u001b[0m\n",
      "\u001b[38;21m[INFO] 12:36: [UCBVI[worker: -1]] | max_global_step = 19 | episode_rewards = 0.01 | n_visited_states = 96 |  \u001b[0m\n",
      "\u001b[38;21m[INFO] 12:41: [UCBVI[worker: -1]] | max_global_step = 20 | episode_rewards = 0.02 | n_visited_states = 101 |  \u001b[0m\n",
      "\u001b[38;21m[INFO] 12:46: [UCBVI[worker: -1]] | max_global_step = 21 | episode_rewards = 0.01 | n_visited_states = 101 |  \u001b[0m\n",
      "\u001b[38;21m[INFO] 12:51: [UCBVI[worker: -1]] | max_global_step = 22 | episode_rewards = 0.01 | n_visited_states = 103 |  \u001b[0m\n",
      "\u001b[38;21m[INFO] 12:56: [UCBVI[worker: -1]] | max_global_step = 23 | episode_rewards = 0.01 | n_visited_states = 103 |  \u001b[0m\n",
      "\u001b[38;21m[INFO] 13:01: [UCBVI[worker: -1]] | max_global_step = 24 | episode_rewards = 0.12000000000000001 | n_visited_states = 106 |  \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# define the environment GridWorld NRoom\n",
    "env=NRoom(\n",
    "    nrooms=9,\n",
    "    remove_walls=False,\n",
    "    room_size=9,\n",
    "    initial_state_distribution=\"center\", #see if we can change to others\n",
    "    include_traps=True,\n",
    ")\n",
    "horizon=env.observation_space.n\n",
    "T=10000 #number of episodes to play\n",
    "agent=UCBVIAgent(env,gamma=0.999,horizon=horizon)\n",
    "print(\"fitting\")\n",
    "info=agent.fit(budget=T)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.enable_rendering()\n",
    "\n",
    "for _ in range(10):\n",
    "    observation,info=env.reset()\n",
    "    for h in range(horizon):\n",
    "        action=agent.policy(observation)\n",
    "        observation,reward,terminated,truncated,info=env.step(action)\n",
    "        done=terminated or truncated\n",
    "        if done:\n",
    "            break\n",
    "env.render()\n",
    "video=env.save_vedio(\"_video/video_plot_rooms.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define BPI-UCBVI agents\n",
    "from rlberry.agents import AgentWithSimplePolicy\n",
    "from rlberry.utils.writers import DefaultWriter\n",
    "class AdaptiveAgent(AgentWithSimplePolicy):\n",
    "    name=\"BPIUCB\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            env,\n",
    "            discount_factor=0.99,\n",
    "            horizon=100,\n",
    "            **kwargs\n",
    "    ):\n",
    "        super().__init__(env=env,**kwargs)\n",
    "\n",
    "        #check environment\n",
    "        assert isinstance(self.env.observation_space, spaces.Discrete)\n",
    "        assert isinstance(self.env.action_space, spaces.Discrete)\n",
    "\n",
    "        S=env.observation_space.n\n",
    "        A=env.action_space.n\n",
    "        self.gamma=discount_factor #gamma\n",
    "        self.horizon=horizon\n",
    "        # other checks\n",
    "        assert self.gamma >= 0 and self.gamma <= 1.0\n",
    "        if self.horizon is None:\n",
    "            assert self.gamma < 1.0, \"If no horizon is given, gamma must be smaller than 1.\"\n",
    "            self.horizon = int(np.ceil(1.0 / (1.0 - self.gamma)))\n",
    "\n",
    "        self.P_hat=np.ones((S,A,S))*1.0/S #estimated transition kernel\n",
    "        self.visitation_count=np.zeros((S,A,S)) #visitation count\n",
    "\n",
    "        def fit(self,budget,**kwargs):\n",
    "            \"\"\"\n",
    "            The parameter budget can represent the number of steps, the number of episodes etc,\n",
    "            depending on the agent.\n",
    "            * Interact with the environment (self.env);\n",
    "            * Train the agent\n",
    "            * Return useful information\n",
    "            \"\"\"\n",
    "            T=budget\n",
    "            rewards=np.zeros(T)\n",
    "            for t in range(T):\n",
    "                observation,info=self.env.reset()\n",
    "                done=False\n",
    "                while not done:\n",
    "                    action=self.policy(observation)\n",
    "                    next_step,reward,terminated,truncated,info=self.env.step(action)\n",
    "                    #update visitation count and policy\n",
    "                    self.update(observation,action,next_step)\n",
    "                    observation=next_step\n",
    "                    done=terminated or truncated\n",
    "                    rewards[t]+=reward\n",
    "            info={\"episode_rewards\": rewards}\n",
    "            return info\n",
    "        \n",
    "        def eval(self,**kwargs):\n",
    "            \"\"\"\n",
    "            Returns a value corresponding to the evaluation of the agent on the\n",
    "            evaluation environment.\n",
    "\n",
    "            For instance, it can be a Monte-Carlo evaluation of the policy learned in fit().\n",
    "            \"\"\"\n",
    "            return super().eval() #use the eval() from AgentWithSimplePolicy\n",
    "        \n",
    "        def policy(self,observation,explo=True):\n",
    "\n",
    "        \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlberry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
