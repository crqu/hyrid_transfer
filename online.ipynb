{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chengruiqu/anaconda3/envs/rlberry/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import gymnasium.spaces as spaces\n",
    "from rlberry.agents import AgentWithSimplePolicy\n",
    "from rlberry_scool.agents import UCBVIAgent\n",
    "from rlberry_scool.envs import Chain\n",
    "from rlberry_research.envs.benchmarks.grid_exploration.nroom import NRoom\n",
    "from rlberry.manager import(\n",
    "    ExperimentManager,\n",
    "    evaluate_agents,\n",
    "    plot_writer_data,\n",
    "    read_writer_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define BPI-UCBVI agent\n",
    "from rlberry.agents import AgentWithSimplePolicy\n",
    "from rlberry.utils.writers import DefaultWriter\n",
    "class AdaptiveAgent(AgentWithSimplePolicy):\n",
    "    name=\"AdaptiveAgent\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            env,\n",
    "            gamma=1,\n",
    "            horizon=100,\n",
    "            delta=0.1,\n",
    "            varepsilon=0.1,\n",
    "            **kwargs\n",
    "    ):\n",
    "        # init base class\n",
    "        AgentWithSimplePolicy.__init__(self,env=env,**kwargs)\n",
    "\n",
    "        self.gamma=gamma\n",
    "        self.horizon=horizon\n",
    "        self.varepsilon=varepsilon\n",
    "        self.delta=delta\n",
    "        self.initial_state,_=self.env.reset()\n",
    "        self.S=self.env.observation_space.n\n",
    "        self.A=self.env.action_space.n\n",
    "        self.bonus=np.ones((self.S,self.A))*self.horizon #should multiply H\n",
    "        self.P_hat=np.ones((self.S,self.A,self.S))*1.0/self.S #estimated transition kernel\n",
    "        self.Nsas=np.zeros((self.S,self.A,self.S)) #visitation count of (s,a,s')\n",
    "        self.Nsa=np.zeros((self.S,self.A)) #visitation count of (s,a)\n",
    "        self.R_hat=np.zeros((self.S,self.A))\n",
    "        #upper and lower confidence bound\n",
    "        self.q_max=np.zeros((self.horizon,self.S,self.A))\n",
    "        self.q_min=np.zeros((self.horizon,self.S,self.A))\n",
    "        self.v_max=np.zeros((self.horizon,self.S))\n",
    "        self.v_min=np.zeros((self.horizon,self.S))\n",
    "        # self.v=np.zeros((self.horizon,self.S,self.A))\n",
    "        #optimality gap upper bound\n",
    "        self.G=np.ones((self.horizon,self.S,self.A)) #should multiply H\n",
    "\n",
    "        \n",
    "    def fit(self,budget,**kwargs):\n",
    "        \"\"\"\n",
    "        The parameter budget can represent the number of steps, the number of episodes etc,\n",
    "        depending on the agent.\n",
    "        * Interact with the environment (self.env);\n",
    "        * Train the agent\n",
    "        * Return useful information\n",
    "        \"\"\"\n",
    "        T=budget\n",
    "        rewards=np.zeros(T)\n",
    "        for t in range(T):\n",
    "            if (t%100==0):\n",
    "                print(\"Episode\",t)\n",
    "            self.update_value()\n",
    "            # self.update_v()\n",
    "            observation,info=self.env.reset()\n",
    "            done=False\n",
    "            step=0\n",
    "            reward=0\n",
    "            current_state=0\n",
    "            while step<self.horizon:\n",
    "                #terminal state is an absorbing state\n",
    "                if done:\n",
    "                    action=self.policy(current_state,step)\n",
    "                    next_step=current_state\n",
    "                    self.update(current_state,action,next_step,reward)\n",
    "                else:\n",
    "                    # print(\"Step\",step+1)\n",
    "                    action=self.policy(observation,step)\n",
    "                    next_step,reward,terminated,truncated,info=self.env.step(action)\n",
    "                    #update visitation count and policy\n",
    "                    self.update(observation,action,next_step,reward)\n",
    "                    current_state=observation\n",
    "                    observation=next_step\n",
    "                    done=terminated or truncated\n",
    "                rewards[t]+=reward\n",
    "                step+=1\n",
    "            # if self.stop():\n",
    "            #     break\n",
    "        info={\"episode_rewards\": rewards}\n",
    "        # print(self.v_max)\n",
    "        return rewards\n",
    "        \n",
    "    def eval(self,**kwargs):\n",
    "        \"\"\"\n",
    "        Returns a value corresponding to the evaluation of the agent on the\n",
    "        evaluation environment.\n",
    "\n",
    "        For instance, it can be a Monte-Carlo evaluation of the policy learned in fit().\n",
    "        \"\"\"\n",
    "        return super().eval() #use the eval() from AgentWithSimplePolicy\n",
    "    \n",
    "    #calculate bonus\n",
    "    def beta(self,n):\n",
    "        beta = np.log(3*self.S*self.A*self.horizon/self.delta) + self.S*np.log(8*np.exp(1)*(n+1))\n",
    "        return beta\n",
    "    \n",
    "    # update counts\n",
    "    def update(self,s,a,next_state,r):\n",
    "        self.Nsas[s,a,next_state]+=1\n",
    "        self.Nsa[s,a]+=1\n",
    "\n",
    "        n_sa=self.Nsa[s,a]\n",
    "        n_sas=self.Nsas[s,a,:]\n",
    "        self.P_hat[s,a,:]=n_sas/n_sa\n",
    "        self.bonus[s,a]=self.beta(n_sa)/n_sa\n",
    "\n",
    "        self.R_hat[s,a]=r\n",
    "    \n",
    "    # compute empirical variance\n",
    "    def emvar(self,v,s,a):\n",
    "        mean=np.dot(v,self.P_hat[s,a,:])\n",
    "        var=var = np.dot(self.P_hat[s, a, :], (v - mean) ** 2)\n",
    "        return var\n",
    "    \n",
    "    # compute \\hat{p}\\pi*G\n",
    "    def backG(self,s,a,h):\n",
    "        out=0\n",
    "        for i in range(self.S):\n",
    "            out+=self.P_hat[s,a,i]*self.G[h+1,i,self.policy(i,h+1)] #replace with self.policy(h+1,i)\n",
    "        return out\n",
    "    # def backV(self,s,a,h):\n",
    "    #     out=0\n",
    "    #     for i in range(self.S):\n",
    "    #         out+=self.P_hat[s,a,i]*np.max(self.v[h+1,i,:])\n",
    "    #     return out\n",
    "    # def update_v(self):\n",
    "    #     for h in range(self.horizon-1,-1,-1):\n",
    "    #         if h==self.horizon-1:\n",
    "    #             self.v[h,:,:]=self.R_hat+4*(self.Nsa+1)**(-1)\n",
    "    #         else:\n",
    "    #             for s in range(self.S):\n",
    "    #                 for a in range(self.A):\n",
    "    #                     self.v[h,s,a]=self.R_hat[s,a]+4*(self.Nsa[s,a]+1)**(-1)+self.backV(s,a,h)\n",
    "\n",
    "    # update v_max v_min G                  \n",
    "    def update_value(self):\n",
    "        for h in range(self.horizon-1,-1,-1):\n",
    "            if h==self.horizon-1:\n",
    "                for s in range(self.S):\n",
    "                    for a in range(self.A):\n",
    "                        self.q_max[h,s,a]=min(self.horizon,self.R_hat[s,a]+0.0002*14*self.horizon**2*self.bonus[s,a])\n",
    "                        self.q_min[h,s,a]=max(0,self.R_hat[s,a]-0.0002*14*self.horizon**2*self.bonus[s,a])\n",
    "                        self.G[h,s,a]=min(self.horizon,0.0002*35*self.horizon**2*self.bonus[s,a])\n",
    "                    self.v_max[h,s]=np.max(self.q_max[h,s,:])\n",
    "                    self.v_min[h,s]=np.max(self.q_min[h,s,:])\n",
    "            else:\n",
    "                for s in range(self.S):\n",
    "                    for a in range(self.A):\n",
    "                        tmp1=np.dot(self.P_hat[s,a,:],self.v_max[h+1,:])\n",
    "                        tmp2=np.dot(self.P_hat[s,a,:],self.v_min[h+1,:])\n",
    "                        emvarV=self.emvar(self.v_max[h+1,:],s,a)\n",
    "                        self.q_max[h,s,a]=min(self.horizon,self.R_hat[s,a]+0.0002*(3*np.sqrt(emvarV*self.bonus[s,a])+14*self.horizon**2*self.bonus[s,a]+(tmp1-tmp2)/self.horizon)+tmp1)\n",
    "                        self.q_min[h,s,a]=max(0,self.R_hat[s,a]-0.0002*(3*np.sqrt(emvarV*self.bonus[s,a])-14*self.horizon**2*self.bonus[s,a]-(tmp1-tmp2)/self.horizon)+tmp2)\n",
    "                        self.G[h,s,a]=min(self.horizon,0.0002*(6*np.sqrt(emvarV*self.bonus[s,a])+35*self.horizon**2*self.bonus[s,a])+(1+3/self.horizon)*self.backG(s,a,h))\n",
    "                    self.v_max[h,s]=np.max(self.q_max[h,s,:])\n",
    "                    self.v_min[h,s]=np.max(self.q_min[h,s,:])\n",
    "\n",
    "    # define the agent's policy\n",
    "    def policy(self,observation,h):\n",
    "        return self.q_max[h,observation,:].argmax()\n",
    "    \n",
    "    # define stopping rule\n",
    "    def stop(self):\n",
    "        if self.G[0,self.initial_state,self.policy(self.initial_state,0)]<self.varepsilon/2:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def policy2(self,observation,h):\n",
    "        return self.v[h,observation,:].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define HYBRID-TRANSFER agent\n",
    "class HybridTransferAgent(AgentWithSimplePolicy):\n",
    "    name=\"HybridTransferAgent\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            env,\n",
    "            source_count,\n",
    "            source_transition,\n",
    "            gamma=1,\n",
    "            sigma=1,\n",
    "            beta=0.9,\n",
    "            horizon=100,\n",
    "            delta=0.1,\n",
    "            varepsilon=0.1,\n",
    "            **kwargs\n",
    "    ):\n",
    "        # init base class\n",
    "        AgentWithSimplePolicy.__init__(self,env=env,**kwargs)\n",
    "\n",
    "        # basic parameters\n",
    "        self.gamma=gamma\n",
    "        self.horizon=horizon\n",
    "        self.sigma=sigma\n",
    "        self.beta=beta\n",
    "        self.varepsilon=varepsilon\n",
    "        self.delta=delta\n",
    "        self.initial_state,_=self.env.reset()\n",
    "        self.S=self.env.observation_space.n\n",
    "        self.A=self.env.action_space.n\n",
    "\n",
    "        # experiences from a previous task\n",
    "        self.source_count=source_count # dimension (S,A)\n",
    "        self.source_transition=source_transition # dimension (S,A,S)\n",
    "\n",
    "        # shifted area\n",
    "        self.shifted_area=set() # contain tuple (s,a)\n",
    "\n",
    "        # task information\n",
    "        self.bonus=np.ones((self.S,self.A))*self.horizon #should multiply H\n",
    "        self.P_hat=np.ones((self.S,self.A,self.S))*1.0/self.S #estimated transition kernel\n",
    "        self.Nsas=np.zeros((self.S,self.A,self.S)) #visitation count of (s,a,s')\n",
    "        self.Nsa=np.zeros((self.S,self.A)) #visitation count of (s,a)\n",
    "        self.R_hat=np.zeros((self.S,self.A))\n",
    "\n",
    "        # upper and lower confidence bound\n",
    "        self.q_max=np.zeros((self.horizon,self.S,self.A))\n",
    "        self.q_min=np.zeros((self.horizon,self.S,self.A))\n",
    "        self.v_max=np.zeros((self.horizon,self.S))\n",
    "        self.v_min=np.zeros((self.horizon,self.S))\n",
    "\n",
    "        # optimality gap upper bound\n",
    "        self.G=np.ones((self.horizon,self.S,self.A))\n",
    "\n",
    "        # uncertainty function\n",
    "        self.Whsa=np.ones((self.horizon,self.S,self.A))\n",
    "\n",
    "    # update relevant functions\n",
    "    def update(self,s,a,next_state,r):\n",
    "        self.Nsas[s,a,next_state]+=1\n",
    "        self.Nsa[s,a]+=1\n",
    "\n",
    "        n_sa=self.Nsa[s,a]\n",
    "        n_sas=self.Nsas[s,a,:]\n",
    "        self.P_hat[s,a,:]=n_sas/n_sa\n",
    "        self.bonus[s,a]=self.beta(n_sa)/n_sa\n",
    "\n",
    "        self.R_hat[s,a]=r\n",
    "    \n",
    "    def eval(self,**kwargs):\n",
    "        \"\"\"\n",
    "        Returns a value corresponding to the evaluation of the agent on the\n",
    "        evaluation environment.\n",
    "\n",
    "        For instance, it can be a Monte-Carlo evaluation of the policy learned in fit().\n",
    "        \"\"\"\n",
    "        return super().eval() #use the eval() from AgentWithSimplePolicy\n",
    "    \n",
    "    #calculate bonus\n",
    "    def beta(self,n):\n",
    "        beta = np.log(6*self.S*self.A*self.horizon/self.delta) + self.S*np.log(8*np.exp(1)*(n+1))\n",
    "        return beta\n",
    "    \n",
    "    # compute empirical variance\n",
    "    def emvar(self,v,s,a):\n",
    "        mean=np.dot(v,self.P_hat[s,a,:])\n",
    "        var=var = np.dot(self.P_hat[s, a, :], (v - mean) ** 2)\n",
    "        return var\n",
    "    \n",
    "    # compute \\hat{p}\\pi*G\n",
    "    def backG(self,s,a,h):\n",
    "        out=0\n",
    "        for i in range(self.S):\n",
    "            out+=self.P_hat[s,a,i]*self.G[h+1,i,self.policy(i,h+1)] #replace with self.policy(h+1,i)\n",
    "        return out\n",
    "\n",
    "    def backW(self,s,a,h):\n",
    "        out=0\n",
    "        for i in range(self.S):\n",
    "            out+=self.P_hat[s,a,i]*np.max(self.Whsa[h+1,i,:])\n",
    "        return out\n",
    "    #update visitation count and transitons and bonus\n",
    "    def update(self,s,a,next_state,r,recognization_ended):\n",
    "        self.R_hat[s,a]=r\n",
    "        if recognization_ended:\n",
    "            if (s,a) in self.shifted_area:\n",
    "                self.Nsas[s,a,next_state]+=1\n",
    "                self.Nsa[s,a]+=1\n",
    "                n_sa=self.Nsa[s,a]\n",
    "                n_sas=self.Nsas[s,a,:]\n",
    "                self.P_hat[s,a,:]=n_sas/n_sa\n",
    "                self.bonus[s,a]=self.beta(n_sa)/n_sa\n",
    "        else:\n",
    "            self.Nsas[s,a,next_state]+=1\n",
    "            self.Nsa[s,a]+=1\n",
    "            n_sa=self.Nsa[s,a]\n",
    "            n_sas=self.Nsas[s,a,:]\n",
    "            self.P_hat[s,a,:]=n_sas/n_sa\n",
    "            self.bonus[s,a]=self.beta(n_sa)/n_sa\n",
    "\n",
    "    # update v_max v_min G Whsa\n",
    "    def update_value(self,recognization_ended):\n",
    "        # phase 2\n",
    "        if recognization_ended:\n",
    "            for h in range(self.horizon-1,-1,-1):\n",
    "                if h==self.horizon-1:\n",
    "                    for s in range(self.S):\n",
    "                        for a in range(self.A):\n",
    "                            self.q_max[h,s,a]=min(self.horizon,self.R_hat[s,a]+0.0002*14*self.horizon*self.bonus[s,a])\n",
    "                            self.q_min[h,s,a]=max(0,self.R_hat[s,a]-0.0002*14*self.horizon*self.bonus[s,a])\n",
    "                            self.G[h,s,a]=min(self.horizon,0.0002*35*self.horizon*self.bonus[s,a])\n",
    "                        self.v_max[h,s]=np.max(self.q_max[h,s,:])\n",
    "                        self.v_min[h,s]=np.max(self.q_min[h,s,:])\n",
    "                else:\n",
    "                    for s in range(self.S):\n",
    "                        for a in range(self.A):\n",
    "                            tmp1=np.dot(self.P_hat[s,a,:],self.v_max[h+1,:])\n",
    "                            tmp2=np.dot(self.P_hat[s,a,:],self.v_min[h+1,:])\n",
    "                            emvarV=self.emvar(self.v_max[h+1,:],s,a)\n",
    "                            self.q_max[h,s,a]=min(self.horizon,self.R_hat[s,a]+0.0002*(3*np.sqrt(emvarV*self.bonus[s,a])+14*self.horizon*self.bonus[s,a]+(tmp1-tmp2)/self.horizon)+tmp1)\n",
    "                            self.q_min[h,s,a]=max(0,self.R_hat[s,a]-0.0002*(3*np.sqrt(emvarV*self.bonus[s,a])-14*self.horizon*self.bonus[s,a]-(tmp1-tmp2)/self.horizon)+tmp2)\n",
    "                            self.G[h,s,a]=min(self.horizon,0.0002*(6*np.sqrt(emvarV*self.bonus[s,a])+35*self.horizon*self.bonus[s,a])+(1+3/self.horizon)*self.backG(s,a,h))\n",
    "                        self.v_max[h,s]=np.max(self.q_max[h,s,:])\n",
    "                        self.v_min[h,s]=np.max(self.q_min[h,s,:])\n",
    "        #phase 1\n",
    "        else:\n",
    "            for h in range(self.horizon-1,-1,-1):\n",
    "                if h==self.horizon-1:\n",
    "                    for s in range(self.S):\n",
    "                        for a in range(self.A):\n",
    "                            self.Whsa[h,s,a]=min(1,0.0004*self.horizon*self.bonus[s,a])\n",
    "                else:\n",
    "                    for s in range(self.S):\n",
    "                        for a in range(self.A):\n",
    "                            self.Whsa[h,s,a]=min(1,0.0004*self.horizon*self.bonus[s,a]+self.backW(s,a,h))\n",
    "    \n",
    "    def fit(self,budget,**kwargs):\n",
    "        T=budget\n",
    "        rewards=np.zeros(T)\n",
    "        recognization_ended=False\n",
    "        for t in range(T):\n",
    "            if(t%100)==0:\n",
    "                print(\"Episode\",t)\n",
    "            # decide whether to stop phase 1\n",
    "            if not recognization_ended:\n",
    "                recognization_ended=self.stop_recognization()\n",
    "            self.update_value(recognization_ended)\n",
    "            observation,info=self.env.reset()\n",
    "            done=False\n",
    "            step=0\n",
    "            reward=0\n",
    "            current_state=0\n",
    "            while step<self.horizon:\n",
    "                if done:\n",
    "                    action=self.policy(current_state,step,recognization_ended)\n",
    "                    next_step,reward,terminated,truncated,info=self.env.step(action)\n",
    "                    #update visitation count and policy\n",
    "                    self.update(observation,action,next_step,reward,recognization_ended)\n",
    "                    current_state=observation\n",
    "                    observation=next_step\n",
    "                    done=terminated or truncated\n",
    "                rewards[t]+=reward\n",
    "                step+=1\n",
    "        return  rewards,recognization_ended\n",
    "    \n",
    "    def policy(self,observation,step,recognization_ended):\n",
    "        if recognization_ended:\n",
    "            return self.q_max[h,observation,:].argmax()\n",
    "        else:\n",
    "            return self.Whsa[step,observation,:].argmax()\n",
    "    \n",
    "    def stop_recognization(self):\n",
    "        initial_action=self.Whsa[0,self.initial_state,:].argmax()\n",
    "        W_max=self.Whsa[0,self.initial_state,initial_action]\n",
    "        error_bound=3*np.sqrt(W_max)+4*W_max\n",
    "        ended=False\n",
    "        if error_bound<self.sigma*self.beta/4:\n",
    "            print(\"recognization ended\")\n",
    "            ended=True\n",
    "            # get the estimated shifted area\n",
    "            for s in range(self.S):\n",
    "                for a in range(self.A):\n",
    "                    TV=0\n",
    "                    for i in range(self.S):\n",
    "                        TV+=np.abs(self.P_hat[s,a,i]-self.source_transition[s,a,i])/2\n",
    "                    if TV>self.beta:\n",
    "                        # there is a shift\n",
    "                        self.shifted_area.add((s,a))\n",
    "                    else:\n",
    "                        # there isn't a shift \n",
    "                        nsa=self.source_count[s,a]\n",
    "                        self.Nsa[s,a]=self.source_count[s,a]\n",
    "                        self.bonus[s,a]=self.beta(nsa)/nsa\n",
    "                        self.P_hat[s,a,:]=self.source_transition[s,a,:]\n",
    "        return ended\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define RF-agent\n",
    "class RFAgent(AgentWithSimplePolicy):\n",
    "    name=\"RFAgent\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            env,\n",
    "            gamma=1,\n",
    "            horizon=100,\n",
    "            delta=0.1,\n",
    "            varepsilon=0.1,\n",
    "            **kwargs\n",
    "    ):\n",
    "        # init base class\n",
    "        AgentWithSimplePolicy.__init__(self,env=env,**kwargs)\n",
    "\n",
    "        # basic parameters\n",
    "        self.gamma=gamma\n",
    "        self.horizon=horizon\n",
    "        self.varepsilon=varepsilon\n",
    "        self.delta=delta\n",
    "        self.initial_state,_=self.env.reset()\n",
    "        self.S=self.env.observation_space.n\n",
    "        self.A=self.env.action_space.n\n",
    "\n",
    "        #task information\n",
    "        self.R_hat=np.zeros((self.S,self.A)) # reward info\n",
    "        self.bonus=np.ones((self.S,self.A))*self.horizon #should multiply H\n",
    "        self.P_hat=np.ones((self.S,self.A,self.S))*1.0/self.S #estimated transition kernel\n",
    "        self.Nsas=np.zeros((self.S,self.A,self.S)) #visitation count of (s,a,s')\n",
    "        self.Nsa=np.zeros((self.S,self.A)) #visitation count of (s,a)\n",
    "\n",
    "        # uncertainty function\n",
    "        self.Whsa=np.ones((self.horizon,self.S,self.A))\n",
    "\n",
    "    def eval(self,**kwargs):\n",
    "        \"\"\"\n",
    "        Returns a value corresponding to the evaluation of the agent on the\n",
    "        evaluation environment.\n",
    "\n",
    "        For instance, it can be a Monte-Carlo evaluation of the policy learned in fit().\n",
    "        \"\"\"\n",
    "        return super().eval() #use the eval() from AgentWithSimplePolicy\n",
    "    \n",
    "    #calculate bonus\n",
    "    def beta(self,n):\n",
    "        beta = np.log(6*self.S*self.A*self.horizon/self.delta) + self.S*np.log(8*np.exp(1)*(n+1))\n",
    "        return beta\n",
    "    \n",
    "    def update(self,s,a,next_state,reward):\n",
    "        self.Nsas[s,a,next_state]+=1\n",
    "        self.Nsa[s,a]+=1\n",
    "\n",
    "        n_sa=self.Nsa[s,a]\n",
    "        n_sas=self.Nsas[s,a,:]\n",
    "        self.P_hat[s,a,:]=n_sas/n_sa\n",
    "        self.R_hat[s,a]=reward\n",
    "        self.bonus[s,a]=self.beta(n_sa)/n_sa\n",
    "\n",
    "    def backW(self,s,a,h):\n",
    "        out=0\n",
    "        for i in range(self.S):\n",
    "            out+=self.P_hat[s,a,i]*np.max(self.Whsa[h+1,i,:])\n",
    "        return out\n",
    "\n",
    "    def update_value(self):\n",
    "        for h in range(self.horizon-1,-1,-1):\n",
    "            if h==self.horizon-1:\n",
    "                for s in range(self.S):\n",
    "                    for a in range(self.A):\n",
    "                        self.Whsa[h,s,a]=min(1,0.0004*self.horizon*self.bonus[s,a])\n",
    "            else:\n",
    "                for s in range(self.S):\n",
    "                    for a in range(self.A):\n",
    "                        self.Whsa[h,s,a]=min(1,0.0004*self.horizon*self.bonus[s,a]+self.backW(s,a,h))\n",
    "\n",
    "    def policy(self,observation,step):\n",
    "        return self.Whsa[step,observation,:].argmax()\n",
    "    \n",
    "    def fit(self,budget,**kwargs):\n",
    "        T=budget\n",
    "        for t in range(T):\n",
    "            if (t%100)==0:\n",
    "                print(\"Episode\",t)\n",
    "            self.update_value()\n",
    "            observation,info=self.env.reset()\n",
    "            done=False\n",
    "            step=0\n",
    "            reward=0\n",
    "            current_state=0\n",
    "            while step<self.horizon:\n",
    "                # terminal state is an absorbing state\n",
    "                if done:\n",
    "                    action=self.policy(current_state,step)\n",
    "                    next_step=current_state\n",
    "                    self.update(current_state,action,next_step,reward)\n",
    "                else:\n",
    "                    action=self.policy(observation,step)\n",
    "                    next_step,reward,terminated,truncated,info=self.env.step(action)\n",
    "                    #update visitation count and policy\n",
    "                    self.update(observation,action,next_step,reward)\n",
    "                    current_state=observation\n",
    "                    observation=next_step\n",
    "                    done=terminated or truncated\n",
    "                step+=1\n",
    "        \n",
    "        np.save(\"source_counts.npy\",self.Nsa)\n",
    "        np.save(\"source_transition.npy\",self.P_hat)\n",
    "        return self.Nsa,self.P_hat\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\n",
      "Episode 100\n",
      "Episode 200\n",
      "Episode 300\n",
      "Episode 400\n",
      "Episode 500\n",
      "Episode 600\n",
      "Episode 700\n",
      "Episode 800\n",
      "Episode 900\n",
      "Episode 1000\n",
      "Episode 1100\n",
      "Episode 1200\n",
      "Episode 1300\n",
      "Episode 1400\n",
      "Episode 1500\n",
      "Episode 1600\n",
      "Episode 1700\n",
      "Episode 1800\n",
      "Episode 1900\n",
      "Episode 2000\n",
      "Episode 2100\n",
      "Episode 2200\n",
      "Episode 2300\n",
      "Episode 2400\n",
      "Episode 2500\n",
      "Episode 2600\n",
      "Episode 2700\n",
      "Episode 2800\n",
      "Episode 2900\n",
      "Episode 3000\n",
      "Episode 3100\n",
      "Episode 3200\n",
      "Episode 3300\n",
      "Episode 3400\n",
      "Episode 3500\n",
      "Episode 3600\n",
      "Episode 3700\n",
      "Episode 3800\n",
      "Episode 3900\n",
      "Episode 4000\n",
      "Episode 4100\n",
      "Episode 4200\n",
      "Episode 4300\n",
      "Episode 4400\n",
      "Episode 4500\n",
      "Episode 4600\n",
      "Episode 4700\n",
      "Episode 4800\n",
      "Episode 4900\n",
      "Episode 5000\n",
      "Episode 5100\n",
      "Episode 5200\n",
      "Episode 5300\n",
      "Episode 5400\n",
      "Episode 5500\n",
      "Episode 5600\n",
      "Episode 5700\n",
      "Episode 5800\n",
      "Episode 5900\n",
      "Episode 6000\n",
      "Episode 6100\n",
      "Episode 6200\n",
      "Episode 6300\n",
      "Episode 6400\n",
      "Episode 6500\n",
      "Episode 6600\n",
      "Episode 6700\n",
      "Episode 6800\n",
      "Episode 6900\n",
      "Episode 7000\n",
      "Episode 7100\n",
      "Episode 7200\n",
      "Episode 7300\n",
      "Episode 7400\n",
      "Episode 7500\n",
      "Episode 7600\n",
      "Episode 7700\n",
      "Episode 7800\n",
      "Episode 7900\n",
      "Episode 8000\n",
      "Episode 8100\n",
      "Episode 8200\n",
      "Episode 8300\n",
      "Episode 8400\n",
      "Episode 8500\n",
      "Episode 8600\n",
      "Episode 8700\n",
      "Episode 8800\n",
      "Episode 8900\n",
      "Episode 9000\n",
      "Episode 9100\n",
      "Episode 9200\n",
      "Episode 9300\n",
      "Episode 9400\n",
      "Episode 9500\n",
      "Episode 9600\n",
      "Episode 9700\n",
      "Episode 9800\n",
      "Episode 9900\n"
     ]
    }
   ],
   "source": [
    "# define the environment GridWorld NRoom\n",
    "env=NRoom(\n",
    "    nrooms=5,\n",
    "    remove_walls=False,\n",
    "    room_size=2,\n",
    "    initial_state_distribution=\"center\", #see if we can change to others\n",
    "    include_traps=True,\n",
    ")\n",
    "\n",
    "horizon=env.observation_space.n\n",
    "T=10000 #number of episodes to play\n",
    "N=2\n",
    "source_count=np.load(\"source_counts.npy\")\n",
    "source_P=np.load(\"source_transition.npy\")\n",
    "# rewards=[]\n",
    "agent_Transfer=HybridTransferAgent(env=env,horizon=horizon,source_count=source_count,source_transition=source_P)\n",
    "agent=RFAgent(env,gamma=1,horizon=horizon)\n",
    "rewards=agent.fit(budget=T)\n",
    "# print(\"fitting \",i+1,\"th agent\")\n",
    "# count,P_hat=agent.fit(budget=T)\n",
    "# for i in range(N):\n",
    "#     agent=AdaptiveAgent(env,gamma=1,horizon=horizon)\n",
    "#     print(\"fitting \",i+1,\"th agent\")\n",
    "#     reward=agent.fit(budget=T)\n",
    "#     rewards.append(np.array(reward))\n",
    "\n",
    "# rewards=np.array(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot routine\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import sem  # 标准误差\n",
    "from scipy.interpolate import make_interp_spline\n",
    "def plot(N,rewards,T):\n",
    "    mean_reward=np.mean(rewards,axis=0)\n",
    "    std_err=sem(rewards,axis=0) #calculate standard error\n",
    "\n",
    "    #calculate confidence interval\n",
    "    confidence_interval=1.96*std_err #95%\n",
    "\n",
    "    x=np.arange(T)\n",
    "\n",
    "    # smoothen the curve\n",
    "    x_smooth = np.linspace(x.min(), x.max(), 300) \n",
    "    spl_mean = make_interp_spline(x, mean_reward, k=3)  # B样条插值\n",
    "\n",
    "    y_smooth = spl_mean(x_smooth)\n",
    "\n",
    "    spl_ci_upper = make_interp_spline(x, mean_reward + confidence_interval, k=3)\n",
    "    y_smooth_upper = spl_ci_upper(x_smooth)\n",
    "\n",
    "    spl_ci_lower = make_interp_spline(x, mean_reward - confidence_interval, k=3)\n",
    "    y_smooth_lower = spl_ci_lower(x_smooth)\n",
    "\n",
    "\n",
    "    # 绘图\n",
    "    # 绘制均值曲线\n",
    "    plt.plot(x_smooth, y_smooth, label='Mean Reward')\n",
    "    # 绘制置信区间的阴影\n",
    "    plt.fill_between(x_smooth, y_smooth_lower, y_smooth_upper, color='b', alpha=0.2, label='95% CI')\n",
    "    plt.title('Episode reward')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Reward')\n",
    "    # plt.legend()\n",
    "    plt.grid(True)  # 添加网格\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(N,rewards,T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.enable_rendering()\n",
    "\n",
    "for _ in range(5):\n",
    "    observation,info=env.reset()\n",
    "    for h in range(horizon):\n",
    "        action=agent_Transfer.policy(observation,h,recognization_ended=True)\n",
    "        observation,reward,terminated,truncated,info=env.step(action)\n",
    "        done=terminated or truncated\n",
    "        if done:\n",
    "            break\n",
    "env.render()\n",
    "video=env.save_video(\"_video/video_plot_rooms.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# chain of length 10. With proba 0.1, the agent will not be able to take the action it wants to take.\n",
    "ucbvi_params={'gamma':1,\"horizon\":14,\"writer_extra\":\"reward\"}\n",
    "random_params={\"writer_extra\":\"reward\"}\n",
    "env_kwargs=dict(nrooms=3,remove_walls=False,room_size=2,initial_state_distribution=\"center\",include_traps=True)\n",
    "ucbvi_stats=ExperimentManager(\n",
    "    AdaptiveAgent,\n",
    "    (NRoom,env_kwargs),\n",
    "    fit_budget=10000,\n",
    "    eval_kwargs=dict(eval_horizon=14,n_simulation=10),\n",
    "    init_kwargs=ucbvi_params,\n",
    "    n_fit=1,\n",
    "    agent_name=\"AdaptiveAgent\",\n",
    ")\n",
    "ucbvi_stats.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of the reward.\n",
    "output = plot_writer_data(\n",
    "    [ucbvi_stats],\n",
    "    tag=\"reward\",\n",
    "    title=\"Episode Reward\",\n",
    ")\n",
    "\n",
    "\n",
    "# Plot of the reward.\n",
    "output = plot_writer_data(\n",
    "    [ucbvi_stats],\n",
    "    tag=\"reward\",\n",
    "    smooth=True,\n",
    "    title=\"Episode Reward smoothed\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlberry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
